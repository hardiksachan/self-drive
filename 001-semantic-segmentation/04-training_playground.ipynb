{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "workspace_root = os.environ[\"SELF_DRIVE_CARLA_WORKSPACE\"]\n",
    "project_workspace = os.path.join(workspace_root, \"01-semantic-segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Import your SegmentationDataset class\n",
    "from src.segmentation_dataset import SegmentationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform train test validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_dataset_path= os.path.join(project_workspace, \"reduced_dataset.csv\")\n",
    "df = pd.read_csv(full_dataset_path, nrows=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = [i % 10 < 7 for i in range(len(df))]\n",
    "train_df = df[train_mask]\n",
    "train_df.to_csv(os.path.join(project_workspace, \"reduced_train_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mask = [7 <= i % 10 < 9 for i in range(len(df))]\n",
    "val_df = df[val_mask]\n",
    "val_df.to_csv(os.path.join(project_workspace, \"reduced_val_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask = [i % 10 == 9 for i in range(len(df))]\n",
    "test_df = df[test_mask]\n",
    "test_df.to_csv(os.path.join(project_workspace, \"reduced_test_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 15\n",
    "\n",
    "# Define model (DeepLabV3+ with pre-trained encoder)\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnet34\",  # Choose appropriate encoder (e.g., resnet50, efficientnet-b0)\n",
    "    encoder_weights=\"imagenet\",  # Load pre-trained weights from ImageNet\n",
    "    classes=n_classes,  # Number of classes in your dataset\n",
    ")\n",
    "\n",
    "# Define loss function (e.g., CrossEntropyLoss for multi-class segmentation)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0) \n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define a transform sequence for image data (adjust as needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize RGB channels\n",
    "    # Add other transformations like random cropping, flipping, etc. (optional)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegmentationDataset(os.path.join(project_workspace, \"reduced_train_dataset.csv\"), project_workspace,mode=\"train\", transform=transform)\n",
    "val_dataset = SegmentationDataset(os.path.join(project_workspace, \"reduced_val_dataset.csv\"), project_workspace, transform=transform)\n",
    "\n",
    "print(len(train_dataset))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "\n",
    "# Initialize the metrics\n",
    "mean_accuracy = MulticlassAccuracy(ignore_index=0, num_classes=n_classes).cuda()\n",
    "miou = MulticlassJaccardIndex(num_classes=n_classes).cuda()\n",
    "\n",
    "writer = SummaryWriter(os.path.join(project_workspace, \"runs/first_2k_try2\"))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epoch = 0\n",
    "\n",
    "while epoch <= 50:\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        predictions = model(images)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mean_accuracy(predictions, labels)\n",
    "        miou(predictions, labels)\n",
    "\n",
    "    # Compute the metrics after all batches are processed\n",
    "    mean_acc = mean_accuracy.compute()\n",
    "    iou = miou.compute()\n",
    "\n",
    "    # Log the metrics after each epoch\n",
    "    writer.add_scalar('Loss', loss.item(), epoch)\n",
    "    writer.add_scalar('Mean Accuracy', mean_acc, epoch)\n",
    "    writer.add_scalar('Mean IoU', iou, epoch)\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(model.state_dict(), os.path.join(project_workspace, f\"models/first_2k_try2/model_epoch_{epoch}.pth\"))\n",
    "    epoch += 1\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()    \n",
    "\n",
    "\n",
    "\n",
    "test_image = val_dataset[7][0].unsqueeze(0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_image)\n",
    "\n",
    "predicted_mask = test_output.squeeze().argmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from src.labels import trainId2label\n",
    "import numpy as np\n",
    "\n",
    "# Convert predicted mask to RGB image\n",
    "predicted_mask_rgb = np.zeros((480, 640, 3))\n",
    "for h in range(480):\n",
    "  for w in range(640):\n",
    "    class_label = predicted_mask[h, w]\n",
    "    predicted_mask_rgb[h, w] = trainId2label[class_label.item()].color\n",
    "\n",
    "predicted_mask_rgb = predicted_mask_rgb / 255.0\n",
    "\n",
    "# Predicted mask (RGB)\n",
    "plt.imshow(predicted_mask_rgb)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
